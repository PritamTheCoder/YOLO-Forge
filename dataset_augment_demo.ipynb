{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb0eb2e",
   "metadata": {},
   "source": [
    "# YOLO Dataset Augmentation Pipeline - Interactive Demo\n",
    "## ----------------------------------------------\n",
    "\n",
    "This notebook demonstrates the comprehensive YOLO dataset augmentation system\n",
    "with bbox-focused transformations for object detection tasks.\n",
    "\n",
    "Author: Pritam Thapa\n",
    "Purpose: Showcase augmentation capabilities and visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0ef31",
   "metadata": {},
   "source": [
    "## YOLO Dataset Augmentation Pipeline Demo\n",
    "\n",
    "## This notebook demonstrates:\n",
    "## 1. **Dataset Preparation**: Scanning, converting, repairing labels\n",
    "## 2. **Bbox-Focused Augmentations**: 8 custom transform types\n",
    "## 3. **Quality Control**: Validation and filtering\n",
    "## 4. **Pipeline Automation**: End-to-end workflow\n",
    "# \n",
    "## Key Features\n",
    "## - Bbox-localized transformations (don't affect background)\n",
    "## - Sport-specific augmentations (ball detection, motion blur, occlusion)\n",
    "## - Realistic lighting effects (stadium lights, shadows, reflections)\n",
    "## - Quality validation (black frame detection, bbox integrity)\n",
    "## - Automatic target-based generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e533c44",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72746ec0",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python numpy pyyaml tqdm albumentations matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12376037",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from pathlib import Path\n",
    "import random\n",
    "import yaml\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa91af5",
   "metadata": {},
   "source": [
    "## Helper functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    \"\"\"Load image in RGB format\"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB) if img is not None else None\n",
    "\n",
    "def load_yolo_labels(label_path):\n",
    "    \"\"\"Load YOLO format labels\"\"\"\n",
    "    bboxes, labels = [], []\n",
    "    if Path(label_path).exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    labels.append(int(parts[0]))\n",
    "                    bboxes.append([float(x) for x in parts[1:5]])\n",
    "    return bboxes, labels\n",
    "\n",
    "def yolo_to_pixels(bbox, img_shape):\n",
    "    \"\"\"Convert YOLO format to pixel coordinates\"\"\"\n",
    "    h, w = img_shape[:2]\n",
    "    x_center, y_center, width, height = bbox\n",
    "    x1 = int((x_center - width / 2) * w)\n",
    "    y1 = int((y_center - height / 2) * h)\n",
    "    x2 = int((x_center + width / 2) * w)\n",
    "    y2 = int((y_center + height / 2) * h)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def draw_bboxes(ax, image, bboxes, color='lime', linewidth=2):\n",
    "    \"\"\"Draw bounding boxes on image\"\"\"\n",
    "    ax.imshow(image)\n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = yolo_to_pixels(bbox, image.shape)\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                        linewidth=linewidth, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.axis('off')\n",
    "\n",
    "def visualize_comparison(original, augmented, bboxes, title=\"Augmentation Result\"):\n",
    "    \"\"\"Side-by-side comparison of original and augmented images\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    draw_bboxes(axes[0], original, bboxes, color='lime')\n",
    "    axes[0].set_title('Original', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    draw_bboxes(axes[1], augmented, bboxes, color='cyan')\n",
    "    axes[1].set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_grid(images, titles, bboxes_list, cols=3):\n",
    "    \"\"\"Display multiple augmented versions in a grid\"\"\"\n",
    "    n = len(images)\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
    "    axes = axes.flatten() if n > 1 else [axes]\n",
    "    \n",
    "    for idx, (img, title, bboxes) in enumerate(zip(images, titles, bboxes_list)):\n",
    "        draw_bboxes(axes[idx], img, bboxes, color='yellow')\n",
    "        axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e2c90",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation Demo\n",
    "### Dataset Structure Detection and Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATASET PREPARATION UTILITIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example: Scan dataset structure\n",
    "def demo_scan_dataset(dataset_path):\n",
    "    \"\"\"Demonstrate dataset scanning\"\"\"\n",
    "    from src.yolo_augmentor.data.scan_dataset import scan_dataset\n",
    "    \n",
    "    print(f\"\\n Scanning dataset: {dataset_path}\")\n",
    "    result = scan_dataset(dataset_path)\n",
    "    \n",
    "    print(f\"\\n Scan Results:\")\n",
    "    print(f\"   Total Images: {result['total_images']}\")\n",
    "    print(f\"   Total Labels: {result['total_labels']}\")\n",
    "    print(f\"   Missing Pairs: {result['missing_pairs']}\")\n",
    "    print(f\"   Structure Type: {result['structure_type']}\")\n",
    "    \n",
    "    if result['has_problems']:\n",
    "        print(f\"\\n  Issues detected:\")\n",
    "        print(f\"   First few missing labels: {result['missing_label_files'][:5]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment to run on your dataset)\n",
    "# demo_scan_dataset(\"path/to/your/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8471df",
   "metadata": {},
   "source": [
    "### Label Repair and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74897cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_label_repair():\n",
    "    \"\"\"Demonstrate label repair capabilities\"\"\"\n",
    "    print(\"\\nüîß LABEL REPAIR CAPABILITIES\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Automatically fixes:\")\n",
    "    print(\"  ‚úì Malformed coordinate formats\")\n",
    "    print(\"  ‚úì Out-of-range bbox values (clips to [0,1])\")\n",
    "    print(\"  ‚úì Zero/negative bbox dimensions\")\n",
    "    print(\"  ‚úì Invalid class labels\")\n",
    "    print(\"\\nExample fixes:\")\n",
    "    \n",
    "    examples = [\n",
    "        (\"0 0.5 0.5 1.2 0.8\", \"0 0.500000 0.500000 1.000000 0.800000\", \"Clipped width > 1.0\"),\n",
    "        (\"0 0.5 0.5 0 0.3\", \"REMOVED\", \"Zero width detected\"),\n",
    "        (\"0 -0.1 0.5 0.3 0.4\", \"0 0.000000 0.500000 0.300000 0.400000\", \"Clipped negative x\"),\n",
    "    ]\n",
    "    \n",
    "    for original, fixed, reason in examples:\n",
    "        print(f\"\\n  Original: {original}\")\n",
    "        print(f\"  Fixed:    {fixed}\")\n",
    "        print(f\"  Reason:   {reason}\")\n",
    "\n",
    "demo_label_repair()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8686f",
   "metadata": {},
   "source": [
    "## 2. Custom Augmentation Transforms\n",
    "### This section demonstrates all 8 custom bbox-focused augmentation types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85f94a",
   "metadata": {},
   "source": [
    "### Transform Categories:\n",
    "### 1. **Motion Simulation**: Shear + blur for fast-moving objects\n",
    "### 2. **Occlusion**: Realistic object hiding/overlap\n",
    "### 3. **Lighting Effects**: Stadium lights, reflections, shadows\n",
    "### 4. **Degradation**: Noise, pixel dropout, compression artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Test Image\n",
    "\n",
    "def create_sample_image():\n",
    "    \"\"\"Create a sample image with a ball-like object for testing\"\"\"\n",
    "    img = np.ones((480, 640, 3), dtype=np.uint8) * 120  # Gray background\n",
    "    \n",
    "    # Add some texture to background\n",
    "    noise = np.random.randint(-20, 20, (480, 640, 3))\n",
    "    img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Draw a ball (circular object)\n",
    "    center = (320, 240)\n",
    "    radius = 40\n",
    "    cv2.circle(img, center, radius, (255, 200, 50), -1)  # Yellow ball\n",
    "    cv2.circle(img, center, radius, (200, 150, 30), 3)   # Border\n",
    "    \n",
    "    # Add highlight\n",
    "    cv2.circle(img, (center[0]-10, center[1]-10), 12, (255, 255, 200), -1)\n",
    "    \n",
    "    # YOLO format bbox for the ball\n",
    "    x_center = center[0] / 640\n",
    "    y_center = center[1] / 480\n",
    "    width = (radius * 2) / 640\n",
    "    height = (radius * 2) / 480\n",
    "    bbox = [x_center, y_center, width, height]\n",
    "    \n",
    "    return img, [bbox], [0]\n",
    "\n",
    "# Create sample\n",
    "sample_img, sample_bboxes, sample_labels = create_sample_image()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "draw_bboxes(ax, sample_img, sample_bboxes, color='lime')\n",
    "ax.set_title('Sample Test Image with Bbox', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47829",
   "metadata": {},
   "source": [
    "## 3. Augmentation Showcase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f35a216",
   "metadata": {},
   "source": [
    "### Transform 1: BboxMultiBlurAndShearTransform\n",
    "### **Purpose**: Simulate fast-moving objects (e.g., pickle ball in flight)\n",
    "\n",
    "### **Effects**:\n",
    "### - Random blur types (Gaussian, Median, Bilateral, Defocus)\n",
    "### - Aggressive shear transformation\n",
    "### - Motion trail simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb8145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the path to your project's root directory (adjust path as necessary)\n",
    "# For example, if the notebook is two levels deep from the root:\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..')) \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa64faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Working Directory (Your Root):\", os.getcwd())\n",
    "print(\"Files in this directory:\", os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import BboxMultiBlurAndShearTransform\n",
    "\n",
    "print(\"\\nüéØ Transform 1: Bbox Multi-Blur + Shear\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Fast-moving ball detection\")\n",
    "print(\"Effects: Motion blur + directional shear\")\n",
    "\n",
    "# Create multiple variations\n",
    "transform = BboxMultiBlurAndShearTransform(\n",
    "    blur_types=['gaussian', 'median', 'defocus'],\n",
    "    blur_strength_range=(5, 15),\n",
    "    shear_x_range=(-80, 80),\n",
    "    shear_y_range=(-40, 40),\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "titles = []\n",
    "\n",
    "for i in range(6):\n",
    "    aug_img, aug_boxes, aug_labels = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "    titles.append(f'Variation {i+1}')\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images[:5], \n",
    "               ['Original'] + titles[:5],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff968c",
   "metadata": {},
   "source": [
    "## Transform 2: BboxExtremeShearOcclude\n",
    " \n",
    "### **Purpose**: Extreme motion + occlusion + brightness changes\n",
    " \n",
    "### **Effects**:\n",
    "### - Extreme shear angles\n",
    "### - Brightness multiplication\n",
    "### - Random occlusion patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import BboxExtremeShearOcclude\n",
    "\n",
    "print(\"\\nüéØ Transform 2: Extreme Shear + Occlusion + Brightness\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Challenging detection scenarios\")\n",
    "print(\"Effects: Extreme motion + lighting + occlusion\")\n",
    "\n",
    "transform = BboxExtremeShearOcclude(\n",
    "    shear_x_range=(-100, 100),\n",
    "    shear_y_range=(-60, 60),\n",
    "    brightness_shift=(1.5, 2.5),\n",
    "    occlusion_intensity='medium',\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Extreme {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40ac1c",
   "metadata": {},
   "source": [
    "## Transform 3: NearBboxExtremeBrighten\n",
    "\n",
    "### **Purpose**: Stadium lighting effects, flash reflections\n",
    " \n",
    "### **Effects**:\n",
    "### - Rectangular halo around bbox (not inside)\n",
    "### - Asymmetric expansion\n",
    "### - Color temperature variations (warm/cool/white)\n",
    "### - Edge-only illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import NearBboxExtremeBrighten\n",
    "\n",
    "print(\"\\nüéØ Transform 3: Near-Bbox Extreme Brightening\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Stadium lights, camera flash, directional lighting\")\n",
    "print(\"Effects: Bright halos around objects, not inside\")\n",
    "\n",
    "transform = NearBboxExtremeBrighten(\n",
    "    expand_horizontal_range=(0.3, 2.0),\n",
    "    expand_vertical_range=(0.3, 2.0),\n",
    "    intensity_range=(1.4, 2.5),\n",
    "    decay='gaussian',\n",
    "    color_bias='auto',\n",
    "    edge_only_prob=0.3,\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "titles = []\n",
    "\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "    titles.append(f'Lighting {i+1}')\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + titles,\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bebe69c",
   "metadata": {},
   "source": [
    "## Transform 4: ConcentratedNoiseTransform\n",
    "\n",
    "### **Purpose**: Sensor noise, low-light conditions\n",
    "### \n",
    "### **Effects**:\n",
    "### - Intense Gaussian noise at bbox center\n",
    "### - Radial falloff to background\n",
    "### - Simulates camera noise under challenging conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import ConcentratedNoiseTransform\n",
    "\n",
    "print(\"\\nüéØ Transform 4: Concentrated Noise\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Low-light photography, high ISO noise\")\n",
    "print(\"Effects: Strong noise at object center, fading outward\")\n",
    "\n",
    "transform = ConcentratedNoiseTransform(\n",
    "    sigma_center=(25, 60),\n",
    "    sigma_background=(3, 15),\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Noise {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb81299",
   "metadata": {},
   "source": [
    "## Transform 5: BallBlendAndShapeBiasTransform\n",
    "\n",
    "### **Purpose**: Shape deformation and background blending\n",
    " \n",
    "### **Effects**:\n",
    "### - Non-uniform scaling (squash/stretch)\n",
    "### - Background color blending\n",
    "### - Simulates camera distortion or motion compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab139f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import BallBlendAndShapeBiasTransform\n",
    "\n",
    "print(\"\\nüéØ Transform 5: Ball Blend + Shape Bias\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Perspective distortion, rolling motion\")\n",
    "print(\"Effects: Shape warping + background color blending\")\n",
    "\n",
    "transform = BallBlendAndShapeBiasTransform(\n",
    "    warp_strength=(0.05, 0.25),\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Warp {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988c99a",
   "metadata": {},
   "source": [
    "## Transform 6: BallPixelLevelOcclusion\n",
    "\n",
    "### **Purpose**: Micro-occlusions, compression artifacts\n",
    " \n",
    "### **Effects**:\n",
    "### - Random pixel dropout\n",
    "### - Salt-and-pepper noise effect\n",
    "### - Simulates video compression or transmission errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import BallPixelLevelOcclusion\n",
    "\n",
    "print(\"\\nüéØ Transform 6: Ball Pixel-Level Occlusion\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Video compression artifacts, transmission errors\")\n",
    "print(\"Effects: Random pixel dropout with noise\")\n",
    "\n",
    "transform = BallPixelLevelOcclusion(\n",
    "    dropout_frac=(0.02, 0.25),\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Dropout {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a7ffa",
   "metadata": {},
   "source": [
    "## Transform 7: GradientPatchTransform\n",
    " \n",
    "### **Purpose**: Smooth directional lighting effects\n",
    " \n",
    "### **Effects**:\n",
    "### - Rectangular gradient overlays\n",
    "### - Directional vs. omnidirectional\n",
    "### - Color temperature shifts\n",
    "### - Simulates natural/artificial light sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdcfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import GradientPatchTransform\n",
    "\n",
    "print(\"\\nüéØ Transform 7: Gradient Patch\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Directional lighting, shadows, reflections\")\n",
    "print(\"Effects: Smooth rectangular gradients around bbox\")\n",
    "\n",
    "transform = GradientPatchTransform(\n",
    "    expand_horizontal_range=(0.5, 1.8),\n",
    "    expand_vertical_range=(0.5, 1.8),\n",
    "    intensity_range=(0.6, 2.0),\n",
    "    color_shift_scale=0.3,\n",
    "    blend_profile='gaussian',\n",
    "    directional_prob=0.4,\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Gradient {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a743bcc",
   "metadata": {},
   "source": [
    "## Transform 8: BboxGaussianOccludeShearTransform\n",
    " \n",
    "### **Purpose**: Fog-like occlusion with motion\n",
    " \n",
    "### **Effects**:\n",
    "### - Gaussian-shaped occlusion masks\n",
    "### - Semi-transparent overlays\n",
    "### - Optional shear and blur\n",
    "### - Simulates fog, haze, or atmospheric effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4890e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.yolo_augmentor.aug.custom_transforms import BboxGaussianOccludeShearTransform\n",
    "\n",
    "print(\"\\nüéØ Transform 8: Gaussian Occlude + Shear\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Use Case: Fog, haze, partial transparency under motion\")\n",
    "print(\"Effects: Smooth Gaussian occlusion + optional shear\")\n",
    "\n",
    "transform = BboxGaussianOccludeShearTransform(\n",
    "    num_patches=(1, 3),\n",
    "    intensity_range=(0.5, 1.5),\n",
    "    shear_x_range=(-50, 50),\n",
    "    shear_y_range=(-20, 20),\n",
    "    color_mode='background',\n",
    "    blur_after=True,\n",
    "    bbox_prob=1.0\n",
    ")\n",
    "\n",
    "augmented_images = []\n",
    "for i in range(5):\n",
    "    aug_img, _, _ = transform.apply(\n",
    "        sample_img.copy(), sample_bboxes.copy(), sample_labels.copy()\n",
    "    )\n",
    "    augmented_images.append(aug_img)\n",
    "\n",
    "visualize_grid([sample_img] + augmented_images, \n",
    "               ['Original'] + [f'Fog {i+1}' for i in range(5)],\n",
    "               [sample_bboxes] * 6,\n",
    "               cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e8417",
   "metadata": {},
   "source": [
    "## 4. Quality Control Demonstration\n",
    " \n",
    "### The pipeline includes automatic quality validation:\n",
    "### - Black frame detection\n",
    "### - Extreme brightness filtering\n",
    "### - Bbox integrity validation\n",
    "### - Transform failure handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72283215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç QUALITY CONTROL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qc_features = {\n",
    "    \"Black Frame Detection\": {\n",
    "        \"threshold\": \"90% dark pixels\",\n",
    "        \"action\": \"Discard augmentation\"\n",
    "    },\n",
    "    \"Brightness Validation\": {\n",
    "        \"range\": \"[3, 252] mean brightness\",\n",
    "        \"action\": \"Reject extreme values\"\n",
    "    },\n",
    "    \"Bbox Integrity\": {\n",
    "        \"checks\": \"Size, coordinates, boundaries\",\n",
    "        \"action\": \"Filter invalid boxes\"\n",
    "    },\n",
    "    \"Transform Failures\": {\n",
    "        \"handling\": \"Graceful fallback\",\n",
    "        \"logging\": \"Detailed error tracking\"\n",
    "    },\n",
    "    \"Post-Augmentation\": {\n",
    "        \"random_discard\": \"5-10% target rate\",\n",
    "        \"purpose\": \"Prevent overfitting\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for feature, details in qc_features.items():\n",
    "    print(f\"\\n‚úì {feature}:\")\n",
    "    for key, value in details.items():\n",
    "        print(f\"  ‚Ä¢ {key.title()}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b4a9ae",
   "metadata": {},
   "source": [
    "## 5. Configuration System\n",
    "\n",
    "### The pipeline uses YAML configuration for flexible control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66919bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è CONFIGURATION EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "example_config = \"\"\"\n",
    "dataset:\n",
    "  input_images_dir: \"dataset/images\"\n",
    "  input_labels_dir: \"dataset/labels\"\n",
    "  output_images_dir: \"dataset_aug/images\"\n",
    "  output_labels_dir: \"dataset_aug/labels\"\n",
    "  target_total_images: 200\n",
    "\n",
    "quality_control:\n",
    "  black_frame_threshold: 0.90\n",
    "  min_brightness: 3\n",
    "  max_brightness: 252\n",
    "  target_discard_rate: 0.08\n",
    "\n",
    "validation:\n",
    "  min_bbox_width: 0.01\n",
    "  min_bbox_height: 0.01\n",
    "  min_visibility: 0.3\n",
    "  coord_tolerance: 0.02\n",
    "\n",
    "augment_passes:\n",
    "  - name: \"bbox_multi_blur_shear\"\n",
    "    type: \"custom\"\n",
    "    weight: 1.5\n",
    "    pipeline:\n",
    "      - transform: \"BboxMultiBlurAndShearTransform\"\n",
    "        params:\n",
    "          blur_types: [\"gaussian\", \"median\", \"defocus\"]\n",
    "          blur_strength_range: [5, 15]\n",
    "          shear_x_range: [-80, 80]\n",
    "          bbox_prob: 0.8\n",
    "  \n",
    "  - name: \"near_bbox_extreme_bright\"\n",
    "    type: \"custom\"\n",
    "    weight: 0.8\n",
    "    pipeline:\n",
    "      - transform: \"NearBboxExtremeBrighten\"\n",
    "        params:\n",
    "          intensity_range: [1.4, 2.5]\n",
    "          edge_only_prob: 0.3\n",
    "\"\"\"\n",
    "print(example_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb7e8d",
   "metadata": {},
   "source": [
    "## 6. Pipeline Statistics\n",
    "\n",
    "### Based on the provided log file, here's what the pipeline achieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5562513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pipeline_stats():\n",
    "    \"\"\"Display statistics from actual pipeline run\"\"\"\n",
    "    stats = {\n",
    "        \"Input Dataset\": {\n",
    "            \"Original Images\": 14,\n",
    "            \"Labels Found\": \"14/14 (100%)\"\n",
    "        },\n",
    "        \"Target Configuration\": {\n",
    "            \"Target Total\": 200,\n",
    "            \"Required Generation\": 186\n",
    "        },\n",
    "        \"Augmentation Passes\": {\n",
    "            \"bbox_multi_blur_shear\": 39,\n",
    "            \"bbox_extreme_shear_bright_occlude\": 33,\n",
    "            \"near_bbox_extreme_bright\": 16,\n",
    "            \"concentrated_noise_center\": 17,\n",
    "            \"ball_blend_shape_bias\": 19,\n",
    "            \"ball_pixel_occlusion\": 22,\n",
    "            \"gradient_patches_near_bbox\": 10,\n",
    "            \"gaussian_occlude_shear\": 28\n",
    "        },\n",
    "        \"Final Results\": {\n",
    "            \"Generated Images\": 184,\n",
    "            \"Random Discard\": 15,\n",
    "            \"Final Total\": 198,\n",
    "            \"Success Rate\": \"100% (0 failures)\"\n",
    "        },\n",
    "        \"Quality Metrics\": {\n",
    "            \"Black Frames\": 0,\n",
    "            \"Extreme Brightness\": 0,\n",
    "            \"Invalid Bboxes\": 0,\n",
    "            \"Transform Failures\": 0,\n",
    "            \"Overall Discard Rate\": \"0.0%\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä PIPELINE EXECUTION STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for section, data in stats.items():\n",
    "        print(f\"\\n{section}:\")\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {data}\")\n",
    "\n",
    "display_pipeline_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58478751",
   "metadata": {},
   "source": [
    "## 7. Complete Pipeline Usage\n",
    "\n",
    "### Full Automation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ COMPLETE PIPELINE WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline_steps = \"\"\"\n",
    "# Step 1: Scan Dataset\n",
    "yolo-forge scan --path dataset/\n",
    "\n",
    "# Step 2: Convert to YOLO Format (if needed)\n",
    "yolo-forge convert --input dataset_raw/ --output dataset_yolo/\n",
    "\n",
    "# Step 3: Repair Labels\n",
    "yolo-forge repair --labels dataset_yolo/labels/\n",
    "\n",
    "# Step 4: Run Augmentation\n",
    "yolo-forge augment --config config_aug.yaml\n",
    "\n",
    "# Step 5: Split Train/Val/Test\n",
    "yolo-forge split --input dataset_aug/ --output dataset_final/ \\\\\n",
    "                 --train 0.8 --val 0.1 --test 0.1\n",
    "\n",
    "# OR: Run Complete Pipeline\n",
    "yolo-forge pipeline --config pipeline_config.yaml\n",
    "\"\"\"\n",
    "\n",
    "print(pipeline_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e805c3",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tips = {\n",
    "    \"Augmentation Intensity\": [\n",
    "        \"Start with conservative parameters\",\n",
    "        \"Gradually increase based on validation performance\",\n",
    "        \"Monitor discard rates (aim for <10%)\"\n",
    "    ],\n",
    "    \"Quality Control\": [\n",
    "        \"Black frame threshold: 0.85-0.95 (not too strict)\",\n",
    "        \"Brightness range: [5, 250] is safer than [3, 252]\",\n",
    "        \"Always enable bbox validation\"\n",
    "    ],\n",
    "    \"Transform Selection\": [\n",
    "        \"Use motion transforms for sports/action\",\n",
    "        \"Use lighting transforms for indoor/outdoor variation\",\n",
    "        \"Use occlusion for crowded scenes\"\n",
    "    ],\n",
    "    \"Target Settings\": [\n",
    "        \"Set realistic target_total_images\",\n",
    "        \"Use weights to balance transform distribution\",\n",
    "        \"Enable random post-discard (5-8%)\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"Process in batches for large datasets\",\n",
    "        \"Enable logging for debugging\",\n",
    "        \"Backup original data before pipeline\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in tips.items():\n",
    "    print(f\"\\n‚úì {category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  ‚Ä¢ {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafdff5",
   "metadata": {},
   "source": [
    "## 9. Real-World Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ REAL-WORLD APPLICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "use_cases = {\n",
    "    \"Sports Analytics\": {\n",
    "        \"objects\": \"Balls, players, equipment\",\n",
    "        \"challenges\": \"Fast motion, blur, occlusion\",\n",
    "        \"recommended\": [\"BboxMultiBlurAndShear\", \"AdaptiveMotionBlur\", \n",
    "                       \"BboxOcclusion\"]\n",
    "    },\n",
    "    \"Autonomous Vehicles\": {\n",
    "        \"objects\": \"Vehicles, pedestrians, signs\",\n",
    "        \"challenges\": \"Weather, lighting, distance\",\n",
    "        \"recommended\": [\"ConcentratedNoise\", \"NearBboxExtremeBrighten\",\n",
    "                       \"GradientPatch\"]\n",
    "    },\n",
    "    \"Retail/Inventory\": {\n",
    "        \"objects\": \"Products on shelves\",\n",
    "        \"challenges\": \"Occlusion, varied lighting, angles\",\n",
    "        \"recommended\": [\"BboxOcclusion\", \"BboxColorJitter\",\n",
    "                       \"BallBlendShape\"]\n",
    "    },\n",
    "    \"Medical Imaging\": {\n",
    "        \"objects\": \"Lesions, organs, instruments\",\n",
    "        \"challenges\": \"Low contrast, noise, artifacts\",\n",
    "        \"recommended\": [\"ConcentratedNoise\", \"BallPixelOcclusion\",\n",
    "                       \"GaussianOcclude\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "for domain, info in use_cases.items():\n",
    "    print(f\"\\n{domain}:\")\n",
    "    print(f\"  Objects: {info['objects']}\")\n",
    "    print(f\"  Challenges: {info['challenges']}\")\n",
    "    print(f\"  Recommended Transforms:\")\n",
    "    for transform in info['recommended']:\n",
    "        print(f\"    - {transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fb244",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08607b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n KEY TAKEAWAYS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = \"\"\"\n",
    "- Comprehensive Pipeline:\n",
    "   ‚Ä¢ End-to-end automation from raw data to train-ready splits\n",
    "   ‚Ä¢ Built-in quality control and validation\n",
    "   ‚Ä¢ Detailed logging and statistics\n",
    "\n",
    "- Bbox-Focused Augmentations:\n",
    "   ‚Ä¢ 8 custom transforms designed for object detection\n",
    "   ‚Ä¢ Localized effects (don't destroy background context)\n",
    "   ‚Ä¢ Sport/action-specific augmentations\n",
    "\n",
    "- Production-Ready:\n",
    "   ‚Ä¢ Robust error handling and graceful degradation\n",
    "   ‚Ä¢ Configurable via YAML (no code changes needed)\n",
    "   ‚Ä¢ Extensive validation and sanity checks\n",
    "\n",
    "- Proven Results:\n",
    "   ‚Ä¢ 100% success rate on test dataset\n",
    "   ‚Ä¢ 0% discard rate (all augmentations valid)\n",
    "   ‚Ä¢ Near-perfect target achievement (198 vs 200)\n",
    "\n",
    "- Flexible Architecture:\n",
    "   ‚Ä¢ Modular transform system\n",
    "   ‚Ä¢ Easy to add new transforms\n",
    "   ‚Ä¢ CLI + Python API support\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
